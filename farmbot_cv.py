# -*- coding: utf-8 -*-
"""Farmbot CV.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1up2qqvKI8oMTE_tSkF9OSFNU4b5bvsqQ

# To install database (from kaggle)
"""

# Install Kaggle API and set up credentials
from google.colab import files
import os

# Upload your kaggle.json file
print("Upload your kaggle.json file.")
files.upload()  # Upload your kaggle.json file here

# Move kaggle.json to the correct location
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Install Kaggle API
!pip install -q kaggle

# Download the Lettuce Disease dataset from Kaggle
!kaggle datasets download -d ashishjstar/lettuce-diseases

# Extract the dataset
!unzip lettuce-diseases.zip -d lettuce_disease_data

"""# Binary segmentation of data into healthy and non-healthy"""

import shutil
import random

# Define source and target directories
source_dir = "/content/lettuce_disease_data/Lettuce_disease_datasets"
train_dir = "/content/lettuce_disease_data/train"
validation_dir = "/content/lettuce_disease_data/validation"
test_dir = "/content/lettuce_disease_data/test"

# Create directories for binary classification.
# Basicamente crea los folders para poner las imagenes clasificadas.
os.makedirs(f"{train_dir}/healthy", exist_ok=True)
os.makedirs(f"{train_dir}/non_healthy", exist_ok=True)
os.makedirs(f"{validation_dir}/healthy", exist_ok=True)
os.makedirs(f"{validation_dir}/non_healthy", exist_ok=True)
os.makedirs(f"{test_dir}/healthy", exist_ok=True)
os.makedirs(f"{test_dir}/non_healthy", exist_ok=True)

# Split the dataset into train, validation, and test sets
healthy_classes = ["Healthy"]  # Specify the class for healthy lettuce

# Go through the files
for disease_class in os.listdir(source_dir):
    disease_class_path = os.path.join(source_dir, disease_class)
    if os.path.isdir(disease_class_path):
        images = os.listdir(disease_class_path)
        random.shuffle(images)

        train_split = int(0.8 * len(images))
        val_split = int(0.9 * len(images))

        # Determine target class (healthy or non_healthy)
        target_class = "healthy" if disease_class in healthy_classes else "non_healthy"

        # Move images to respective directories
        for img in images[:train_split]:
            shutil.copy(os.path.join(disease_class_path, img), f"{train_dir}/{target_class}")
        for img in images[train_split:val_split]:
            shutil.copy(os.path.join(disease_class_path, img), f"{validation_dir}/{target_class}")
        for img in images[val_split:]:
            shutil.copy(os.path.join(disease_class_path, img), f"{test_dir}/{target_class}")

"""# Data generators"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Data augmentation applied to the training set (data normalization)
train_datagen = ImageDataGenerator(
    rescale=1.0/255,              # Normalize pixel values to [0, 1]
    rotation_range=40,            # Randomly rotate images by up to 40 degrees
    width_shift_range=0.2,        # Randomly shift images horizontally by 20% of width
    height_shift_range=0.2,       # Randomly shift images vertically by 20% of height
    shear_range=0.2,              # Randomly shear images
    zoom_range=0.2,               # Randomly zoom in on images
    horizontal_flip=True,         # Randomly flip images horizontally
    brightness_range=[0.8, 1.2],  # Randomly adjust brightness
    fill_mode='nearest'           # Fill in missing pixels after transformations
)

# Validation and test sets only rescale images (no augmentation)
validation_datagen = ImageDataGenerator(rescale=1.0/255)
test_datagen = ImageDataGenerator(rescale=1.0/255)

# Create generators
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='binary'  # Binary classification
)

validation_generator = validation_datagen.flow_from_directory(
    validation_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='binary'
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='binary',
    shuffle=False
)

"""MODEL"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Build the CNN model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),
    MaxPooling2D(2, 2),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')  # Binary classification
])

# Compile the model
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    epochs=15,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // validation_generator.batch_size,
    callbacks=[early_stopping]
)

"""Summary"""

# Plot training and validation accuracy/loss
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(len(acc))

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Predict on the test set
Y_pred = model.predict(test_generator)
y_pred = (Y_pred > 0.5).astype(int)  # Convert probabilities to binary classes

# Generate the confusion matrix
cm = confusion_matrix(test_generator.classes, y_pred)
cm_labels = ["Healthy", "Non-Healthy"]

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=cm_labels, yticklabels=cm_labels)
plt.title("Confusion Matrix")
plt.ylabel("Actual")
plt.xlabel("Predicted")
plt.show()

# Print classification report
print("Classification Report:")
print(classification_report(test_generator.classes, y_pred, target_names=cm_labels))

print("Class Distribution in Training Set:")
for label, count in train_generator.class_indices.items():
    print(f"{label}: {sum(train_generator.classes == count)} images")

# Save the model
model.save("healthy_vs_non_healthy_classifier.h5")
print("Model saved as healthy_vs_non_healthy_classifier.h5")

"""TEST"""

from google.colab import files
from PIL import Image
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Reload the trained model
model = tf.keras.models.load_model("healthy_vs_non_healthy_classifier.h5")

# Function to classify an uploaded image
def classify_uploaded_image(model, img_path):
    # Load the image using Pillow (PIL)
    img = Image.open(img_path).convert('RGB')  # Convert to RGB to ensure 3 channels
    img = img.resize((150, 150))  # Resize to model's input size
    img_array = np.array(img)  # Convert image to NumPy array
    img_array = img_array / 255.0  # Normalize pixel values to [0, 1]
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension

    # Make predictions
    prediction = model.predict(img_array)
    if prediction[0] > 0.5:
        result = "Non-Healthy"
        confidence = prediction[0][0] * 100
    else:
        result = "Healthy"
        confidence = (1 - prediction[0][0]) * 100

    # Display the image and prediction
    plt.imshow(img)
    plt.axis('off')
    plt.title(f"Predicted: {result} ({confidence:.2f}% confidence)")
    plt.show()

    print(f"The uploaded lettuce is classified as: {result}")
    print(f"Confidence score: {confidence:.2f}%")

# Upload the image
print("Please upload an image:")
uploaded = files.upload()

# Get the uploaded file's path
for filename in uploaded.keys():
    img_path = filename  # Use the filename directly as the path

# Classify the uploaded image
classify_uploaded_image(model, img_path)